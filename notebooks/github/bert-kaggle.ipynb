{"cells":[{"metadata":{"id":"VtfD5gY8_zQM","trusted":true},"cell_type":"code","source":"import os\nimport pickle\nfrom datetime import datetime\n\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom transformers import BertForSequenceClassification, Trainer, TrainingArguments","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loc = os.environ.get('KAGGLE_KERNEL_RUN_TYPE','Localhost')\nif loc == 'Interactive' or loc == 'Localhost':\n    conf = {\n        'train_size': 2000,\n        'test_size': 2000,\n        'epochs': 2,\n    }\nelif loc == 'Batch':\n    conf = {\n        'train_size': -1,\n        'test_size': -1,\n        'epochs': 5\n    }\n    \ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'","execution_count":null,"outputs":[]},{"metadata":{"id":"L0kV2j_D_zQM"},"cell_type":"markdown","source":"# Data tokenization"},{"metadata":{"trusted":true},"cell_type":"code","source":"with open(\"../input/text-classification-tfidf/github/X_train.pickle\", 'rb') as f:\n    X_train = pickle.load(f)\nwith open(\"../input/text-classification-tfidf/github/\"+\"/X_test.pickle\", 'rb') as f:\n    X_test = pickle.load(f)\nwith open(\"../input/text-classification-tfidf/github/\"+\"/y_train.pickle\", 'rb') as f:\n    y_train = pickle.load(f)\nwith open(\"../input/text-classification-tfidf/github/\"+\"/y_test.pickle\", 'rb') as f:\n    y_test = pickle.load(f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(y_train), len(y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_texts = X_train[:conf['train_size']].to_list()\ntrain_labels = torch.tensor(y_train[:conf['train_size']].to_list()).to(device)\ntest_texts = X_test[:conf['test_size']].to_list()\ntest_labels = torch.tensor(y_test[:conf['test_size']].to_list()).to(device)","execution_count":null,"outputs":[]},{"metadata":{"id":"Qyd4nwIe_zRw","outputId":"eaf2959d-2153-42f7-fc6b-1e5eba00fcd4","trusted":true},"cell_type":"code","source":"from transformers import DistilBertTokenizerFast\ntokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')","execution_count":null,"outputs":[]},{"metadata":{"id":"6UK7LCDx_zRw","trusted":true},"cell_type":"code","source":"# If the ``encoded_inputs`` passed are dictionary of numpy arrays, PyTorch tensors or TensorFlow tensors, the\n# result will use the same type unless you provide a different tensor type with ``return_tensors``. In the\n# case of PyTorch tensors, you will lose the specific device of your tensors however.\n# â†’ les texts n'Ã©tant pas des tensors, tokenizer ne renvoyait pas de tensors, et donc le to(device) ne fonctionnait pas\ntrain_encodings = tokenizer(train_texts, truncation=True, padding=True, return_tensors=\"pt\").to(device)\n#val_encodings = tokenizer(val_texts, truncation=True, padding=True, return_tensors=\"pt\").to(device)\ntest_encodings = tokenizer(test_texts, truncation=True, padding=True, return_tensors=\"pt\").to(device)","execution_count":null,"outputs":[]},{"metadata":{"id":"TzR2nEm__zRw"},"cell_type":"markdown","source":"# Datasets"},{"metadata":{"trusted":true},"cell_type":"code","source":"# element_size in bytes\ntrain_encodings.input_ids.element_size()*train_encodings.input_ids.nelement() /10**6","execution_count":null,"outputs":[]},{"metadata":{"id":"_4VlKKD1_zRw","trusted":true},"cell_type":"code","source":"import torch\n\nclass GitDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: val[idx] for key, val in self.encodings.items()}\n        item['labels'] = self.labels[idx]\n        return item\n\n    def __len__(self):\n        return len(self.labels)\n\ntrain_dataset = GitDataset(train_encodings, train_labels)\n#val_dataset = GitDataset(val_encodings, val_labels)\ntest_dataset = GitDataset(test_encodings, test_labels)","execution_count":null,"outputs":[]},{"metadata":{"id":"MqUdRCq0_zRw"},"cell_type":"markdown","source":"# Fine-tuning"},{"metadata":{"id":"Zij6KuFvQSuM","trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n\ndef compute_metrics(pred):\n    labels = pred.label_ids\n    preds = pred.predictions.argmax(-1)\n    #precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n    acc = accuracy_score(labels, preds)\n    return {\n        'accuracy': acc,\n        #'f1': f1,\n        #'precision': precision,\n        #'recall': recall\n    }\n","execution_count":null,"outputs":[]},{"metadata":{"id":"ayOS1NvwXQT3","trusted":true},"cell_type":"code","source":"#model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\nmodel = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3)","execution_count":null,"outputs":[]},{"metadata":{"id":"lPNQBTHQ_zRw","trusted":true},"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir='../tmp/',            # output directory\n    num_train_epochs=conf['epochs'], # total # of training epochs\n    per_device_train_batch_size=8,   # batch size per device during training\n    per_device_eval_batch_size=32,   # batch size for evaluation\n    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n    weight_decay=0.01,               # strength of weight decay\n    logging_dir='./logs',            # directory for storing logs\n    logging_steps=10,\n)\n\ntrainer = Trainer(\n    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n    args=training_args,                  # training arguments, defined above\n    train_dataset=train_dataset,         # training dataset\n    eval_dataset=test_dataset,           # evaluation dataset\n    compute_metrics = compute_metrics\n)","execution_count":null,"outputs":[]},{"metadata":{"id":"oXuEBGb9KdL8","trusted":true},"cell_type":"code","source":"model.to(device)","execution_count":null,"outputs":[]},{"metadata":{"id":"V2yJGp69_zRw","outputId":"1cdd8451-0a4b-49d4-af77-2a8d1ced09fb","trusted":true,"_kg_hide-output":false},"cell_type":"code","source":"trainer.train()","execution_count":null,"outputs":[]},{"metadata":{"id":"rxDQCuZzPqok","outputId":"75b80e65-f127-4077-dc5b-5d16d25d2712","trusted":true},"cell_type":"code","source":"trainer.evaluate()","execution_count":null,"outputs":[]},{"metadata":{"id":"X_WolaZ6OiWA","trusted":true},"cell_type":"code","source":"trainer.save_model('bert_github_kaggle_'+datetime.today().isoformat(timespec='hours'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}