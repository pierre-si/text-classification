{"cells":[{"metadata":{"id":"VtfD5gY8_zQM","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nfrom transformers import BertForSequenceClassification, Trainer, TrainingArguments","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'","execution_count":null,"outputs":[]},{"metadata":{"id":"L0kV2j_D_zQM"},"cell_type":"markdown","source":"# Data tokenization"},{"metadata":{"id":"OHL_4eiq_zQM","trusted":true},"cell_type":"code","source":"train = pd.read_json('../input/text-classification/github/embold_train.json')\ntrain_extra = pd.read_json('../input/text-classification/github/embold_train_extra.json')\ntrain = pd.concat([train, train_extra], ignore_index=True)\ntest = pd.read_json('../input/text-classification/github/embold_test.json')\n# 0: bug, 1: feature, 2: question","execution_count":null,"outputs":[]},{"metadata":{"id":"CqZYIT5Y_zQM","outputId":"fa89ff0f-59a8-4569-b7dd-eed60a92ba24","trusted":true},"cell_type":"code","source":"train['text'] = train.title + ' ' + train.body\ntest['text'] = test.title + ' ' + test.body\n\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(train), len(test))","execution_count":null,"outputs":[]},{"metadata":{"id":"UinllUwh_zQM","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain, val = train_test_split(train, test_size=.2)","execution_count":null,"outputs":[]},{"metadata":{"id":"kGcxBT5D_zRw","trusted":true},"cell_type":"code","source":"train_texts = train[:20_000].text.to_list()#to_numpy()\nval_texts = val[:15_000].text.to_list()#to_numpy()\ntest_texts = val[-15_000:].text.to_list()\n#test_texts = test[:2000].text.to_list()#to_numpy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels = torch.tensor(train[:20_000].label.to_list()).to(device)#to_numpy()\nval_labels = torch.tensor(val[:15_000].label.to_list()).to(device)#to_numpy()\ntest_labels = torch.tensor(val[-15_000:].label.to_list()).to(device)\n#test_labels = test[:2000].label.to_list()#to_numpy()","execution_count":null,"outputs":[]},{"metadata":{"id":"Qyd4nwIe_zRw","outputId":"eaf2959d-2153-42f7-fc6b-1e5eba00fcd4","trusted":true},"cell_type":"code","source":"from transformers import DistilBertTokenizerFast\ntokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')","execution_count":null,"outputs":[]},{"metadata":{"id":"6UK7LCDx_zRw","trusted":true},"cell_type":"code","source":"# If the ``encoded_inputs`` passed are dictionary of numpy arrays, PyTorch tensors or TensorFlow tensors, the\n# result will use the same type unless you provide a different tensor type with ``return_tensors``. In the\n# case of PyTorch tensors, you will lose the specific device of your tensors however.\n# â†’ les texts n'Ã©tant pas des tensors, tokenizer ne renvoyait pas de tensors, et donc le to(device) ne fonctionnait pas\ntrain_encodings = tokenizer(train_texts, truncation=True, padding=True, return_tensors=\"pt\").to(device)\nval_encodings = tokenizer(val_texts, truncation=True, padding=True, return_tensors=\"pt\").to(device)\ntest_encodings = tokenizer(test_texts, truncation=True, padding=True, return_tensors=\"pt\").to(device)","execution_count":null,"outputs":[]},{"metadata":{"id":"TzR2nEm__zRw"},"cell_type":"markdown","source":"# Datasets"},{"metadata":{"trusted":true},"cell_type":"code","source":"# element_size in bytes\ntrain_encodings.input_ids.element_size()*train_encodings.input_ids.nelement() /10**6","execution_count":null,"outputs":[]},{"metadata":{"id":"_4VlKKD1_zRw","trusted":true},"cell_type":"code","source":"import torch\n\nclass GitDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: val[idx] for key, val in self.encodings.items()}\n        item['labels'] = self.labels[idx]\n        return item\n\n    def __len__(self):\n        return len(self.labels)\n\ntrain_dataset = GitDataset(train_encodings, train_labels)\nval_dataset = GitDataset(val_encodings, val_labels)\ntest_dataset = GitDataset(test_encodings, test_labels)","execution_count":null,"outputs":[]},{"metadata":{"id":"MqUdRCq0_zRw"},"cell_type":"markdown","source":"# Fine-tuning"},{"metadata":{"id":"Zij6KuFvQSuM","trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n\ndef compute_metrics(pred):\n    labels = pred.label_ids\n    preds = pred.predictions.argmax(-1)\n    #precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n    acc = accuracy_score(labels, preds)\n    return {\n        'accuracy': acc,\n        #'f1': f1,\n        #'precision': precision,\n        #'recall': recall\n    }\n","execution_count":null,"outputs":[]},{"metadata":{"id":"ayOS1NvwXQT3","trusted":true},"cell_type":"code","source":"#model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\nmodel = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3)","execution_count":null,"outputs":[]},{"metadata":{"id":"lPNQBTHQ_zRw","trusted":true},"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir='./results',          # output directory\n    num_train_epochs=3,              # total # of training epochs\n    per_device_train_batch_size=16,  # batch size per device during training\n    per_device_eval_batch_size=64,   # batch size for evaluation\n    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n    weight_decay=0.01,               # strength of weight decay\n    #logging_dir='./logs',            # directory for storing logs\n    #logging_steps=10,\n)\n\ntrainer = Trainer(\n    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n    args=training_args,                  # training arguments, defined above\n    train_dataset=train_dataset,         # training dataset\n    eval_dataset=test_dataset,           # evaluation dataset\n    compute_metrics = compute_metrics\n)","execution_count":null,"outputs":[]},{"metadata":{"id":"oXuEBGb9KdL8","trusted":true},"cell_type":"code","source":"model.to(device)","execution_count":null,"outputs":[]},{"metadata":{"id":"V2yJGp69_zRw","outputId":"1cdd8451-0a4b-49d4-af77-2a8d1ced09fb","trusted":true,"_kg_hide-output":false},"cell_type":"code","source":"trainer.train()","execution_count":null,"outputs":[]},{"metadata":{"id":"rxDQCuZzPqok","outputId":"75b80e65-f127-4077-dc5b-5d16d25d2712","trusted":true},"cell_type":"code","source":"trainer.evaluate()","execution_count":null,"outputs":[]},{"metadata":{"id":"X_WolaZ6OiWA","trusted":true},"cell_type":"code","source":"trainer.save_model('bert_github_kaggle')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"End\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}